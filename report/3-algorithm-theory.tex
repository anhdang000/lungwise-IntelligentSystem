\section{Algorithm Theory}

In machine learning, classification is a common task that aims to assign a category label to an input data sample based on its features. There are many different classification algorithms, each with its own strengths and weaknesses, suitable for different types of data and problems.

\subsection{Common Classification Algorithms}
Some commonly used classification algorithms include:
\begin{itemize}
    \item \textbf{Logistic Regression \cite{cortes1995support}:} A simple but effective linear model, often used as a baseline. It models the probability of a binary class using the sigmoid function \cite{cox1958regression}.
    \item \textbf{Support Vector Machine (SVM) \cite{cortes1995support}:} Searches for an optimal hyperplane to separate data classes in feature space. SVM is effective in high-dimensional spaces and when the number of dimensions is greater than the number of samples .
    \item \textbf{Decision Tree \cite{Patel_2018}:} Builds a tree-like model where each internal node represents a test on a feature, each branch represents the outcome of the test, and each leaf node represents a class label. Decision trees are easy to interpret but prone to overfitting.
    \item \textbf{Random Forest \cite{breiman2001random}:} An ensemble learning algorithm that builds multiple decision trees during training and outputs the mode of the classes (for classification) or mean prediction (for regression) of the individual trees. It reduces the overfitting problem of single decision trees \cite{breiman2001random}.
    \item \textbf{Naïve Bayes \cite{2000_Frank}:} Based on Bayes' theorem with a "naïve" assumption of conditional independence between features. It is simple, fast, and works well on large datasets.
    \item \textbf{Gradient Boosting Machines (GBM) \cite{friedman2001greedy}:} Another ensemble learning technique that builds models (usually decision trees) sequentially, with each new model trying to correct the errors of previous models. Popular variations include AdaBoost, Gradient Boosting, and XGBoost.
\end{itemize}

\subsection{Choosing XGBoost}

In this project, we chose XGBoost (eXtreme Gradient Boosting) as the main classification algorithm for the following reasons:
\begin{itemize}
    \item \textbf{High Performance:} XGBoost consistently achieves top results in machine learning competitions and is known for high prediction accuracy on tabular data.
    \item \textbf{Optimization and Speed:} The XGBoost library is highly optimized for speed and memory usage. It implements techniques such as parallel computing, handling of missing values, and efficient tree pruning.
    \item \textbf{Overfitting Prevention:} XGBoost incorporates regularization techniques such as L1 (Lasso) and L2 (Ridge) directly into its objective function, helping to minimize overfitting and improve model generalization.
    \item \textbf{Handling Missing Values:} XGBoost has built-in mechanisms to handle missing values in data, simplifying the preprocessing process.
    \item \textbf{Flexibility:} XGBoost supports custom objective and evaluation functions, allowing model fine-tuning for specific problems.
\end{itemize}

Compared to other algorithms, XGBoost provides a good balance between accuracy, speed, and overfitting control, making it especially suitable for the tabular medical data used in this project.

\subsection{Overview of XGBoost}
XGBoost is an optimized implementation of the Gradient Boosting algorithm. Key concepts include:
\begin{itemize}
    \item \textbf{Gradient Boosting:} An ensemble machine learning method that builds prediction models as a combination of weak prediction models (usually decision trees). It builds trees sequentially, with each new tree learning to correct the errors (residuals) of the previous tree set. Unlike AdaBoost which adjusts data point weights, Gradient Boosting adjusts the model based on the gradient of the loss function.
    \item \textbf{Regularization:} To prevent overfitting, XGBoost adds regularization components to its objective function. This includes both L1 regularization (helping reduce the number of features) and L2 regularization (helping reduce the magnitude of leaf weights), making the model generalize better on unseen data.
    \item \textbf{Tree Pruning and Sparsity Awareness:} XGBoost uses an efficient approximation algorithm to find optimal split points and performs tree pruning based on the maximum negative gain. It is also designed to handle missing values in data efficiently by learning default directions at each node.
    \item \textbf{System Optimization:} XGBoost leverages hardware and software optimization techniques such as parallel computing (using all CPU cores), distributed computing, memory cache optimization, and out-of-core computation to process large datasets efficiently.
\end{itemize}
The combination of these techniques helps XGBoost deliver high prediction performance and good scalability for structured tabular data, making it a suitable choice for the early diagnosis of lung cancer.
